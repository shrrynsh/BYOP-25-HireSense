{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10504921,"sourceType":"datasetVersion","datasetId":6503275},{"sourceId":10505065,"sourceType":"datasetVersion","datasetId":6503346}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-18T09:23:17.528280Z","iopub.execute_input":"2025-01-18T09:23:17.528918Z","iopub.status.idle":"2025-01-18T09:23:18.747626Z","shell.execute_reply.started":"2025-01-18T09:23:17.528869Z","shell.execute_reply":"2025-01-18T09:23:18.746516Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/vvvvvvvvvvvvv/jd_ner_model_pickle.pkl\n/kaggle/input/vvvvvvvvvvvvv/elecengineer.pdf\n/kaggle/input/vvvvvvvvvvvvv/resume_ner_model_pickle.pkl\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Installing all the necesarry modules","metadata":{}},{"cell_type":"code","source":"!pip install simpletransformers\n!pip install PyMuPDF\n!pip install sentence_transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T09:23:18.748711Z","iopub.execute_input":"2025-01-18T09:23:18.749283Z","iopub.status.idle":"2025-01-18T09:23:41.993135Z","shell.execute_reply.started":"2025-01-18T09:23:18.749244Z","shell.execute_reply":"2025-01-18T09:23:41.991725Z"}},"outputs":[{"name":"stdout","text":"Collecting simpletransformers\n  Downloading simpletransformers-0.70.1-py3-none-any.whl.metadata (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.32.3)\nRequirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.67.1)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2024.11.6)\nRequirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (3.2.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.13.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.2.2)\nCollecting seqeval (from simpletransformers)\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.17.1)\nCollecting tensorboardx (from simpletransformers)\n  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.2.2)\nRequirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.21.0)\nRequirement already satisfied: wandb>=0.10.32 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.19.1)\nCollecting streamlit (from simpletransformers)\n  Downloading streamlit-1.41.1-py2.py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (0.27.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (6.0.2)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (0.4.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->simpletransformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->simpletransformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->simpletransformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->simpletransformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->simpletransformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->simpletransformers) (2.4.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (3.1.43)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (4.3.6)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (5.9.5)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (2.10.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (2.19.2)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (75.1.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2024.12.14)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->simpletransformers) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (3.11.10)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2024.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (3.5.0)\nRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (5.5.0)\nRequirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (1.9.0)\nRequirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (5.5.0)\nRequirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (11.0.0)\nRequirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (13.9.4)\nRequirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (9.0.0)\nRequirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.10.2)\nRequirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (6.0.0)\nCollecting pydeck<1,>=0.8.0b4 (from streamlit->simpletransformers)\n  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (6.3.3)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.68.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (3.7)\nRequirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.17.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (3.1.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.4)\nRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.23.0)\nRequirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (1.18.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.18.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.11)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb>=0.10.32->simpletransformers) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb>=0.10.32->simpletransformers) (2.27.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.18.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->simpletransformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->simpletransformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->simpletransformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->simpletransformers) (2024.2.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->simpletransformers) (2024.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.22.3)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.2)\nDownloading simpletransformers-0.70.1-py3-none-any.whl (316 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.3/316.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading streamlit-1.41.1-py2.py3-none-any.whl (9.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=a4658f4009c4e85553cca44ed45251acd17b98a03050cc1e8200ac3fe76ba199\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: pydeck, tensorboardx, streamlit, seqeval, simpletransformers\nSuccessfully installed pydeck-0.9.1 seqeval-1.2.2 simpletransformers-0.70.1 streamlit-1.41.1 tensorboardx-2.6.2.2\nCollecting PyMuPDF\n  Downloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\nDownloading pymupdf-1.25.2-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: PyMuPDF\nSuccessfully installed PyMuPDF-1.25.2\nRequirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.47.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.27.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (11.0.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.9.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.12.14)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Importing all the necesarry Libraries","metadata":{}},{"cell_type":"code","source":"import spacy\nimport fitz\nfrom transformers import AutoModelForTokenClassification, AutoTokenizer\nimport torch\nfrom typing import List, Dict\nfrom collections import Counter\nimport pickle\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sentence_transformers import SentenceTransformer\nfrom collections import defaultdict \nfrom transformers import AutoConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T09:23:41.994465Z","iopub.execute_input":"2025-01-18T09:23:41.994922Z","iopub.status.idle":"2025-01-18T09:24:14.832164Z","shell.execute_reply.started":"2025-01-18T09:23:41.994871Z","shell.execute_reply":"2025-01-18T09:24:14.830982Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"Loading the Job Description Parser Model","metadata":{}},{"cell_type":"code","source":"file_size = os.path.getsize('/kaggle/input/vvvvvvvvvvvvv/jd_ner_model_pickle.pkl')\nwith open('/kaggle/input/vvvvvvvvvvvvv/jd_ner_model_pickle.pkl', 'rb') as file:\n  resume_model = pickle.load(file) \nprint(f\"Model loaded successfully! File size: {file_size} bytes\")  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T09:24:14.834853Z","iopub.execute_input":"2025-01-18T09:24:14.835551Z","iopub.status.idle":"2025-01-18T09:24:24.848665Z","shell.execute_reply.started":"2025-01-18T09:24:14.835511Z","shell.execute_reply":"2025-01-18T09:24:24.847451Z"}},"outputs":[{"name":"stdout","text":"Model loaded successfully! File size: 1330618217 bytes\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Loading the Resume Parser Model","metadata":{}},{"cell_type":"code","source":"file_size = os.path.getsize('/kaggle/input/vvvvvvvvvvvvv/resume_ner_model_pickle.pkl')\nwith open('/kaggle/input/vvvvvvvvvvvvv/resume_ner_model_pickle.pkl', 'rb') as file:\n  jd_model = pickle.load(file)\n\nprint(f\"Model loaded successfully! File size: {file_size} bytes\")  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T09:24:24.850895Z","iopub.execute_input":"2025-01-18T09:24:24.851391Z","iopub.status.idle":"2025-01-18T09:24:32.682700Z","shell.execute_reply.started":"2025-01-18T09:24:24.851351Z","shell.execute_reply":"2025-01-18T09:24:32.681198Z"}},"outputs":[{"name":"stdout","text":"Model loaded successfully! File size: 1330560118 bytes\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Defining the Text Preprocessing and pdf  loading functions\n","metadata":{}},{"cell_type":"code","source":"\nnlp=spacy.load(\"en_core_web_sm\")\n\ndef text_preprocess(yo):\n doc=nlp(yo)\n\n string=[]\n\n for token in doc:\n  if token.is_stop==False and token.is_punct==False: # Removing stop words and removing punctuation marks\n     string.append(token.lemma_)  #Lemmatization\n\n return \" \".join(string)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T09:24:32.683772Z","iopub.execute_input":"2025-01-18T09:24:32.684170Z","iopub.status.idle":"2025-01-18T09:24:33.650912Z","shell.execute_reply.started":"2025-01-18T09:24:32.684137Z","shell.execute_reply":"2025-01-18T09:24:33.649816Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def pdf_load(name):\n  pdf_file = name\n  doc = fitz.open(pdf_file)\n\n  text = \" \"\n\n  for page_num in range(doc.page_count):\n      page = doc.load_page(page_num)\n      text += page.get_text()\n\n  return (\" \".join(text.split(\"\\n\")))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T09:24:33.651967Z","iopub.execute_input":"2025-01-18T09:24:33.652259Z","iopub.status.idle":"2025-01-18T09:24:33.657330Z","shell.execute_reply.started":"2025-01-18T09:24:33.652234Z","shell.execute_reply":"2025-01-18T09:24:33.656216Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"Functions to get predictions of annotations of resume and jds","metadata":{}},{"cell_type":"code","source":"class EnsembleNERResume:\n    def initialize(self, pretrained_model_path, device='cuda'):\n       \n        config = AutoConfig.from_pretrained('bert-large-cased')  \n\n       \n        self.bert_model = AutoModelForTokenClassification.from_config(config) \n\n      \n        with open(pretrained_model_path, 'rb') as file:\n       \n            self.bert_model = pickle.load(file)  #loading the pickle model\n         \n\n        self.bert_tokenizer = AutoTokenizer.from_pretrained('bert-large-cased')\n\n        # Device setup\n        self.device = device\n        self.bert_model.to(device)\n        self.bert_model.eval()\n\n\n    def predict(self, text: str) -> List[Dict[str, str]]:\n        # Tokenize and get predictions from the BERT model\n        bert_tokens = self.bert_tokenizer(\n            text,\n            return_tensors=\"pt\",\n            return_offsets_mapping=True,  \n            padding=True,\n            truncation=True\n        ).to(self.device)\n\n       \n        offset_mapping = bert_tokens.pop('offset_mapping')\n\n        with torch.no_grad():\n            bert_outputs = self.bert_model(**bert_tokens)\n            bert_predictions = torch.argmax(bert_outputs.logits, dim=2)[0]\n\n        # Convert predictions to word-level labels\n        word_predictions = []\n        current_word = \"\"\n        current_labels = []\n\n        input_ids = bert_tokens[\"input_ids\"][0]  \n\n        for idx, (start, end) in enumerate(offset_mapping[0]):\n            if start == 0 and end == 0:  \n                continue\n\n       \n            token_text = self.bert_tokenizer.decode(input_ids[idx].item())\n            pred_label = self.bert_model.config.id2label[bert_predictions[idx].item()]\n\n           \n            if token_text.startswith(\"##\"):\n                current_word += token_text[2:]  \n                current_labels.append(pred_label)\n            else:\n                if current_word:\n                   \n                    final_label = Counter(current_labels).most_common(1)[0][0]\n                    word_predictions.append({\"word\": current_word, \"label\": final_label})\n\n               \n                current_word = token_text\n                current_labels = [pred_label]\n\n       \n        if current_word and current_labels:\n            final_label = Counter(current_labels).most_common(1)[0][0]\n            word_predictions.append({\"word\": current_word, \"label\": final_label}) \n\n        return word_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T09:24:33.658676Z","iopub.execute_input":"2025-01-18T09:24:33.659049Z","iopub.status.idle":"2025-01-18T09:24:33.674684Z","shell.execute_reply.started":"2025-01-18T09:24:33.659011Z","shell.execute_reply":"2025-01-18T09:24:33.673547Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class EnsembleNERJd:\n    def initialize(self, pretrained_model_path, device='cuda'):\n       \n        config = AutoConfig.from_pretrained('bert-large-cased')  \n\n      \n        self.bert_model = AutoModelForTokenClassification.from_config(config) # loding the pickle model\n\n       \n        with open(pretrained_model_path, 'rb') as file:\n            \n            self.bert_model = pickle.load(file)  \n            \n\n        self.bert_tokenizer = AutoTokenizer.from_pretrained('bert-large-cased')\n\n        # Device setup\n        self.device = device\n        self.bert_model.to(device)\n        self.bert_model.eval()\n\n\n    def predict(self, text: str) -> List[Dict[str, str]]:\n     \n        bert_tokens = self.bert_tokenizer(\n            text,\n            return_tensors=\"pt\",\n            return_offsets_mapping=True,  \n            padding=True,\n            truncation=True\n        ).to(self.device)\n\n        \n        offset_mapping = bert_tokens.pop('offset_mapping')\n\n        with torch.no_grad():\n            bert_outputs = self.bert_model(**bert_tokens)\n            bert_predictions = torch.argmax(bert_outputs.logits, dim=2)[0]\n\n        # Convert predictions to word-level labels\n        word_predictions = []\n        current_word = \"\"\n        current_labels = []\n\n        input_ids = bert_tokens[\"input_ids\"][0]  \n\n        for idx, (start, end) in enumerate(offset_mapping[0]):\n            if start == 0 and end == 0:  \n                continue\n\n           \n            token_text = self.bert_tokenizer.decode(input_ids[idx].item())\n            pred_label = self.bert_model.config.id2label[bert_predictions[idx].item()]\n\n          \n            if token_text.startswith(\"##\"):\n                current_word += token_text[2:]  \n                current_labels.append(pred_label)\n            else:\n                if current_word:\n                    \n                    final_label = Counter(current_labels).most_common(1)[0][0]\n                    word_predictions.append({\"word\": current_word, \"label\": final_label}) \n\n               \n                current_word = token_text\n                current_labels = [pred_label]\n\n       \n        if current_word and current_labels:\n            final_label = Counter(current_labels).most_common(1)[0][0]\n            word_predictions.append({\"word\": current_word, \"label\": final_label})\n\n        return word_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T09:24:33.675536Z","iopub.execute_input":"2025-01-18T09:24:33.675870Z","iopub.status.idle":"2025-01-18T09:24:33.693469Z","shell.execute_reply.started":"2025-01-18T09:24:33.675842Z","shell.execute_reply":"2025-01-18T09:24:33.692340Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"Functions to convert the prediction in JSON format","metadata":{}},{"cell_type":"code","source":"def group_entities_unique_jd(predictions: List[Dict[str, str]]) -> Dict[str, list]:\n    entities = defaultdict(set)  # Use a set to store unique entities\n    current_entity = []\n    current_label = None\n\n    for pred in predictions:\n        word = pred['word']\n        label = pred['label'].strip() \n\n        if label.startswith(\"B-\"):\n            if current_entity and current_label:\n                entities[current_label].add(\" \".join(current_entity))  \n            current_label = label[2:]  \n            current_entity = [word]  \n        elif label.startswith(\"I-\") and current_label == label[2:]:\n            current_entity.append(word)  \n        elif label.startswith(\"I-\") and current_label != label[2:]:\n            if current_entity and current_label:\n                entities[current_label].add(\" \".join(current_entity))  \n            current_label = label[2:]  \n            current_entity = [word]\n        else:\n            if current_entity and current_label:\n                entities[current_label].add(\" \".join(current_entity))  \n            current_entity = []\n            current_label = None\n\n    if current_entity and current_label:\n        entities[current_label].add(\" \".join(current_entity)) \n\n    # Convert sets back to lists for JSON \n    return {label: list(entity_set) for label, entity_set in entities.items()}\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T09:24:33.694457Z","iopub.execute_input":"2025-01-18T09:24:33.694761Z","iopub.status.idle":"2025-01-18T09:24:33.713250Z","shell.execute_reply.started":"2025-01-18T09:24:33.694734Z","shell.execute_reply":"2025-01-18T09:24:33.712019Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def group_entities_unique_resume(predictions: List[Dict[str, str]]) -> Dict[str, list]:\n\n    entities = defaultdict(set)  # Use a set to store unique entities\n    current_entity = []\n    current_label = None\n\n    for pred in predictions:\n        word = pred['word']\n        label = pred['label']\n\n        if label.startswith(\"B-\"): \n            if current_entity and current_label:\n                entities[current_label].add(\" \".join(current_entity))  \n            current_label = label[2:]  \n            current_entity = [word]\n        elif label.startswith(\"I-\") and current_label == label[2:]: \n            current_entity.append(word)\n        else: \n            if current_entity and current_label:\n                entities[current_label].add(\" \".join(current_entity)) \n            current_entity = []\n            current_label = None\n\n    if current_entity and current_label:\n        entities[current_label].add(\" \".join(current_entity))  \n    # Convert sets back to lists for JSON \n    return {label: list(entity_set) for label, entity_set in entities.items()}\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T09:24:33.714472Z","iopub.execute_input":"2025-01-18T09:24:33.714939Z","iopub.status.idle":"2025-01-18T09:24:33.739034Z","shell.execute_reply.started":"2025-01-18T09:24:33.714796Z","shell.execute_reply":"2025-01-18T09:24:33.737888Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Below few codes loads the pdfs of various resumes and get the annotated resumes","metadata":{}},{"cell_type":"code","source":"text1=text_preprocess(pdf_load(\"/kaggle/input/vvvvvvvvvvvvv/elecengineer.pdf\"))\nensemble = EnsembleNERResume(\n        pretrained_model_path=\"/kaggle/input/vvvvvvvvvvvvv/resume_ner_model_pickle.pkl\",\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n  \npredictions1 = ensemble.predict(text1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T09:44:33.290000Z","iopub.execute_input":"2025-01-18T09:44:33.290446Z","iopub.status.idle":"2025-01-18T09:44:46.881263Z","shell.execute_reply.started":"2025-01-18T09:44:33.290410Z","shell.execute_reply":"2025-01-18T09:44:46.879793Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"text2=text_preprocess(pdf_load(\"/kaggle/input/resumes/animator.pdf\"))\nensemble = EnsembleNERResume(\n        pretrained_model_path=\"/kaggle/input/vvvvvvvvvvvvv/resume_ner_model_pickle.pkl\",\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n  \npredictions2 = ensemble.predict(text2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T09:44:46.885964Z","iopub.execute_input":"2025-01-18T09:44:46.886380Z","iopub.status.idle":"2025-01-18T09:44:56.213682Z","shell.execute_reply.started":"2025-01-18T09:44:46.886343Z","shell.execute_reply":"2025-01-18T09:44:56.212354Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"\ntext3=text_preprocess(pdf_load(\"/kaggle/input/resumes/bartender.pdf\"))\nensemble = EnsembleNERResume(\n        pretrained_model_path=\"/kaggle/input/vvvvvvvvvvvvv/resume_ner_model_pickle.pkl\",\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n  \npredictions3 = ensemble.predict(text3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T09:44:56.216813Z","iopub.execute_input":"2025-01-18T09:44:56.217188Z","iopub.status.idle":"2025-01-18T09:45:05.411148Z","shell.execute_reply.started":"2025-01-18T09:44:56.217156Z","shell.execute_reply":"2025-01-18T09:45:05.410155Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"elecengresume=group_entities_unique_resume(predictions1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T10:29:24.063785Z","iopub.execute_input":"2025-01-18T10:29:24.064385Z","iopub.status.idle":"2025-01-18T10:29:24.072749Z","shell.execute_reply.started":"2025-01-18T10:29:24.064336Z","shell.execute_reply":"2025-01-18T10:29:24.071477Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"animatorresume=group_entities_unique_resume(predictions2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T10:29:24.350275Z","iopub.execute_input":"2025-01-18T10:29:24.350680Z","iopub.status.idle":"2025-01-18T10:29:24.356187Z","shell.execute_reply.started":"2025-01-18T10:29:24.350646Z","shell.execute_reply":"2025-01-18T10:29:24.355122Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"bartenderresume=group_entities_unique_resume(predictions3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T10:29:26.198005Z","iopub.execute_input":"2025-01-18T10:29:26.198387Z","iopub.status.idle":"2025-01-18T10:29:26.203806Z","shell.execute_reply.started":"2025-01-18T10:29:26.198355Z","shell.execute_reply":"2025-01-18T10:29:26.202530Z"}},"outputs":[],"execution_count":82},{"cell_type":"markdown","source":"Parsing and getting the annotations for JOB Description","metadata":{}},{"cell_type":"code","source":"jd=text_preprocess(\"Electrical Engineer,Electronics Hardware Engineers develop and design electronic components and systems, from circuit boards to hardware prototypes, ensuring functionality and performance.\")\nensemble = EnsembleNERJd(\n        pretrained_model_path=\"/kaggle/input/vvvvvvvvvvvvv/jd_ner_model_pickle.pkl\",\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n  \npredictionsjd = ensemble.predict(jd)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T09:41:06.319616Z","iopub.execute_input":"2025-01-18T09:41:06.319982Z","iopub.status.idle":"2025-01-18T09:41:13.419539Z","shell.execute_reply.started":"2025-01-18T09:41:06.319952Z","shell.execute_reply":"2025-01-18T09:41:13.418448Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"elecengjd=group_entities_unique_jd(predictionsjd)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T10:29:31.694509Z","iopub.execute_input":"2025-01-18T10:29:31.694892Z","iopub.status.idle":"2025-01-18T10:29:31.700197Z","shell.execute_reply.started":"2025-01-18T10:29:31.694863Z","shell.execute_reply":"2025-01-18T10:29:31.698829Z"}},"outputs":[],"execution_count":83},{"cell_type":"markdown","source":"Loading the Pre trained T5 model","metadata":{}},{"cell_type":"code","source":"# Load the Sentence-BERT model\nbert_model = SentenceTransformer('sentence-t5-xxl')  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T09:24:47.126677Z","iopub.execute_input":"2025-01-18T09:24:47.127233Z","iopub.status.idle":"2025-01-18T09:26:20.956085Z","shell.execute_reply.started":"2025-01-18T09:24:47.127185Z","shell.execute_reply":"2025-01-18T09:26:20.954642Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1732e364aece4fa7bfb7e19d30b902db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13f3abf21cb549d8a71ace6d284f165c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.98k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"042b36b6caf74487942640e0fa68da6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68e0de974b7a4722a75b73cfeba8d10e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11b8dc1f15a04383a0c9dfbfd60c03e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/9.73G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b967d4cc0bb4ade8b8ce21beddb8e20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7d3bdca7a7746dbbdf51a8760393563"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f1c8e5a818942099f6634ed6f4f2598"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c29137d93381410094384ba328d63c1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02f837d0edc34b03a5c1725b4478db6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d322642f79774ec399c7a8222b8cb629"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"2_Dense/config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"969f734f704244bf8714e3eca802a53e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"878a90a8833a4656b40a31ff4ab191ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.15M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"785a8fd885444584a4dba5f453279feb"}},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"Defing Function to Calculate Cosine Similarity between Various Resumes and Job Descriptions","metadata":{}},{"cell_type":"code","source":"def eval(res,jd):\n\n resume = res# resume dictionary\n job_description =jd  #  job description dictionary\n\n# Preprocess and join fields for embedding generation\n def preprocess_field(field):\n     return \" \".join(field) if field else \"\"\n\n# Define the fields and weights\n fields = {\n    'Skills': {'resume_key': 'Skills', 'jd_key': 'Skill', 'weight': 0.3},\n    'Degree': {'resume_key': 'Degree', 'jd_key': 'Degree', 'weight': 0.2},\n    'Designation': {'resume_key': 'Designation', 'jd_key': 'Designation', 'weight': 0.2},\n    'Experience': {'resume_key': 'Experience', 'jd_key': 'Experience', 'weight': 0.2},\n    'Companies Worked At': {'resume_key': 'Companies Worked At', 'jd_key': 'Companies Worked At', 'weight': 0.1},\n }\n\n\n total_similarity = 0\n total_weight = 0\n\n# Process each field\n for field_name, field_info in fields.items():\n   \n    resume_field = resume.get(field_info['resume_key'], [])\n    jd_field = job_description.get(field_info['jd_key'], [])\n    \n   \n    if resume_field and jd_field:\n        \n        resume_text = preprocess_field(resume_field)\n        jd_text = preprocess_field(jd_field)\n        \n        # Generate embeddings using BERT\n        resume_vector = bert_model.encode(resume_text)\n        jd_vector = bert_model.encode(jd_text)\n        \n       \n        similarity = cosine_similarity([resume_vector], [jd_vector])[0][0]\n    else:\n        \n        similarity = 0\n    \n    \n    weighted_similarity = field_info['weight'] * similarity\n    total_similarity += weighted_similarity\n    total_weight += field_info['weight']\n    \n    \n    print(f\"{field_name} Similarity: {similarity:.4f} (Weighted Contribution: {weighted_similarity:.4f})\")\n\n\n overall_similarity = total_similarity / total_weight if total_weight > 0 else 0\n\n\n print(f\"Overall Similarity: {overall_similarity:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T10:29:36.269640Z","iopub.execute_input":"2025-01-18T10:29:36.270039Z","iopub.status.idle":"2025-01-18T10:29:36.283259Z","shell.execute_reply.started":"2025-01-18T10:29:36.269996Z","shell.execute_reply":"2025-01-18T10:29:36.281398Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"eval(elecengresume,elecengjd)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T10:29:36.856599Z","iopub.execute_input":"2025-01-18T10:29:36.856990Z","iopub.status.idle":"2025-01-18T10:30:05.069685Z","shell.execute_reply.started":"2025-01-18T10:29:36.856954Z","shell.execute_reply":"2025-01-18T10:30:05.068111Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a33980bf31e4dc48d759438e4f09741"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c21d1aa663e49e9bd9bb389199e56c4"}},"metadata":{}},{"name":"stdout","text":"Skills Similarity: 0.7764 (Weighted Contribution: 0.2329)\nDegree Similarity: 0.0000 (Weighted Contribution: 0.0000)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c5a4e26f3db4228b96a13de97922db0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4596397797af47b4a995a886dc3d6a48"}},"metadata":{}},{"name":"stdout","text":"Designation Similarity: 0.8746 (Weighted Contribution: 0.1749)\nExperience Similarity: 0.0000 (Weighted Contribution: 0.0000)\nCompanies Worked At Similarity: 0.0000 (Weighted Contribution: 0.0000)\nOverall Similarity: 0.4078\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"eval(animatorresume,elecengjd)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T10:30:05.071474Z","iopub.execute_input":"2025-01-18T10:30:05.071935Z","iopub.status.idle":"2025-01-18T10:30:27.216181Z","shell.execute_reply.started":"2025-01-18T10:30:05.071899Z","shell.execute_reply":"2025-01-18T10:30:27.214133Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8408d19e80e144509b2022117375cdb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfdebe036a6c4b2b9de6496988961771"}},"metadata":{}},{"name":"stdout","text":"Skills Similarity: 0.6904 (Weighted Contribution: 0.2071)\nDegree Similarity: 0.0000 (Weighted Contribution: 0.0000)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cc1cbedb1a0408dbcb60534811a92c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ce21dc7e9f344498d5abacb5873fe0c"}},"metadata":{}},{"name":"stdout","text":"Designation Similarity: 0.7811 (Weighted Contribution: 0.1562)\nExperience Similarity: 0.0000 (Weighted Contribution: 0.0000)\nCompanies Worked At Similarity: 0.0000 (Weighted Contribution: 0.0000)\nOverall Similarity: 0.3633\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"eval(bartenderresume,elecengjd)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T10:30:27.218533Z","iopub.execute_input":"2025-01-18T10:30:27.218928Z","iopub.status.idle":"2025-01-18T10:30:48.797337Z","shell.execute_reply.started":"2025-01-18T10:30:27.218894Z","shell.execute_reply":"2025-01-18T10:30:48.795830Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cb86bbb2cd148ef8145807599a4fc3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6baacb3e1b34d4a8e7d60486907b882"}},"metadata":{}},{"name":"stdout","text":"Skills Similarity: 0.6801 (Weighted Contribution: 0.2040)\nDegree Similarity: 0.0000 (Weighted Contribution: 0.0000)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"012fb2e5f8c948c6be717d9d1d5a982b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39fc37041e2e4dd585710d83be6d5d3e"}},"metadata":{}},{"name":"stdout","text":"Designation Similarity: 0.7328 (Weighted Contribution: 0.1466)\nExperience Similarity: 0.0000 (Weighted Contribution: 0.0000)\nCompanies Worked At Similarity: 0.0000 (Weighted Contribution: 0.0000)\nOverall Similarity: 0.3506\n","output_type":"stream"}],"execution_count":87},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}